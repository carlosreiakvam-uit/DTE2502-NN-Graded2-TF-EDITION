{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from utils import play_game2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense\n",
    "from tensorflow.keras import Model\n",
    "from agents.agent import Agent\n",
    "from agents.agent import mean_huber_loss\n",
    "from game_environment import SnakeNumpy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Parameters as from the config file of v17.1\n",
    "version = 'v17.1'\n",
    "board_size = 10\n",
    "n_actions = 4\n",
    "frames = 2\n",
    "supervised = False\n",
    "obstacles = False\n",
    "max_time_limit = 998\n",
    "buffer_size = 80000\n",
    "frame_mode = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class DeepQLearningAgent(Agent):\n",
    "    def __init__(self, board_size, frames, buffer_size, n_actions, version, use_target_net=True, gamma=0.99):\n",
    "        super().__init__(board_size, frames, buffer_size, gamma, n_actions, use_target_net, version)\n",
    "        self._model = self.model()\n",
    "        self._target_net = self._model\n",
    "        self.update_target_net()\n",
    "\n",
    "    # Manually implpementing the same model as in the v17.1 setup\n",
    "    def model(self):\n",
    "        input_board = Input((10, 10, 2,), name='input')\n",
    "        model = Sequential()\n",
    "        model.add(input_board)\n",
    "        model.add(Conv2D(filters=16, kernel_size=[3, 3], input_shape=(10, 10, 2),\n",
    "                         activation='relu', data_format='channels_last',\n",
    "                         padding='same', ))\n",
    "        model.add(Conv2D(filters=32, kernel_size=[3, 3], input_shape=(10, 10, 16),\n",
    "                         activation='relu', data_format='channels_last', ))\n",
    "        model.add(Conv2D(filters=64, kernel_size=[5, 5], input_shape=(8,8, 32),\n",
    "                         activation='relu', data_format='channels_last', ))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=64, activation='relu', name='action_prev_dense'))\n",
    "        model.add(Dense(units=4, activation='linear', name='action_values'))\n",
    "        model.compile(optimizer=RMSprop(0.0005), loss=mean_huber_loss)\n",
    "        return model\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self._target_net.set_weights(self._model.get_weights())\n",
    "\n",
    "    def train_agent(self, batch_size=32, num_games=1, reward_clip=False):\n",
    "        s, a, r, next_s, done, legal_moves = self._buffer.sample(batch_size)\n",
    "        if reward_clip:\n",
    "            r = np.sign(r)\n",
    "        # calculate the discounted reward, and then train accordingly\n",
    "        current_model = self._target_net if self._use_target_net else self._model\n",
    "        next_model_outputs = self._get_model_outputs(next_s, current_model)\n",
    "        # our estimate of expexted future discounted reward\n",
    "        discounted_reward = r + (self._gamma * np.max(\n",
    "            np.where(legal_moves == 1, next_model_outputs, -np.inf),\n",
    "            axis=1).reshape(-1, 1)) * (1 - done)\n",
    "        # create the target variable, only the column with action has different value\n",
    "        target = self._get_model_outputs(s)\n",
    "        # we bother only with the difference in reward estimate at the selected action\n",
    "        target = (1 - a) * target + a * discounted_reward\n",
    "        # fit\n",
    "        loss = self._model.train_on_batch(self._normalize_board(s), target)\n",
    "        # loss = round(loss, 5)\n",
    "        return loss\n",
    "\n",
    "    def _get_model_outputs(self, board, model=None):\n",
    "        # to correct dimensions and normalize\n",
    "        board = self._prepare_input(board)\n",
    "        # the default model to use\n",
    "        if model is None:\n",
    "            model = self._model\n",
    "        model_outputs = model.predict_on_batch(board)\n",
    "        return model_outputs\n",
    "\n",
    "    def _prepare_input(self, board):\n",
    "        if (board.ndim == 3):\n",
    "            board = board.reshape((1,) + self._input_shape)\n",
    "        board = self._normalize_board(board.copy())\n",
    "        return board.copy()\n",
    "\n",
    "    def _normalize_board(self, board):\n",
    "        return board.astype(np.float32) / 4.0\n",
    "\n",
    "    def save_model(self, file_path='', iteration=None):\n",
    "        if iteration is not None:\n",
    "            assert isinstance(iteration, int), \"iteration should be an integer\"\n",
    "        else:\n",
    "            iteration = 0\n",
    "        self._model.save_weights(\"{}/model_{:04d}.h5\".format(file_path, iteration))\n",
    "        if self._use_target_net:\n",
    "            self._target_net.save_weights(\"{}/model_{:04d}_target.h5\".format(file_path, iteration))\n",
    "\n",
    "    def move(self, board, legal_moves, value=None):\n",
    "        # use the agent model to make the predictions\n",
    "        model_outputs = self._get_model_outputs(board, self._model)\n",
    "        return np.argmax(np.where(legal_moves == 1, model_outputs, -np.inf), axis=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_27 (Conv2D)           (None, 10, 10, 16)        304       \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 8, 8, 32)          4640      \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 4, 4, 64)          51264     \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "action_prev_dense (Dense)    (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "action_values (Dense)        (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 122,068\n",
      "Trainable params: 122,068\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# This part is pretty much demystified\n",
    "# when DeepQLearningAgent is instanciated, it creates initial models based on the input of the given parameters including version for the model to use.\n",
    "agent = DeepQLearningAgent(\n",
    "    board_size=board_size, frames=frames, buffer_size=buffer_size, n_actions=n_actions,\n",
    "    version=version)\n",
    "agent.model().summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# some more funny parameters\n",
    "epsilon, epsilon_end = 1, 0.01\n",
    "reward_type = 'current'\n",
    "sample_actions = False\n",
    "n_games_training = 8 * 16\n",
    "games_eval = 8\n",
    "decay = 0.97\n",
    "episodes = 1 * (10 ** 3)\n",
    "log_frequency = 250"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing 32768 frames took 1.29s\n"
     ]
    }
   ],
   "source": [
    "# SnakeNumpy and play_game2 are not meant to be altered\n",
    "# In theory They are to be left alone and just work with whatever framework you provide\n",
    "# play some games to fill buffer\n",
    "games = 512\n",
    "env = SnakeNumpy(board_size=board_size, frames=frames,\n",
    "                 max_time_limit=max_time_limit, games=games,\n",
    "                 frame_mode=True, obstacles=obstacles, version=version)\n",
    "ct = time.time()\n",
    "_ = play_game2(env, agent, n_actions, n_games=games, record=True,\n",
    "               epsilon=epsilon, verbose=True, reset_seed=False,\n",
    "               frame_mode=True, total_frames=games * 64)\n",
    "print('Playing {:d} frames took {:.2f}s'.format(games * 64, time.time() - ct))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# For some reason env is reinitialized in the very same manner. This might be an error but should not have significance.\n",
    "env = SnakeNumpy(board_size=board_size, frames=frames,\n",
    "                 max_time_limit=max_time_limit, games=n_games_training,\n",
    "                 frame_mode=True, obstacles=obstacles, version=version)\n",
    "env2 = SnakeNumpy(board_size=board_size, frames=frames,\n",
    "                  max_time_limit=max_time_limit, games=games_eval,\n",
    "                  frame_mode=True, obstacles=obstacles, version=version)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:33<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_11452\\2385512384.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     19\u001B[0m     \u001B[1;31m# First encounter with train_agent\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m     loss = agent.train_agent(batch_size=64,\n\u001B[1;32m---> 21\u001B[1;33m                              num_games=n_games_training, reward_clip=True)\n\u001B[0m\u001B[0;32m     22\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mlog_frequency\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_11452\\1665206245.py\u001B[0m in \u001B[0;36mtrain_agent\u001B[1;34m(self, batch_size, num_games, reward_clip)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mtrain_agent\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m32\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_games\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreward_clip\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 30\u001B[1;33m         \u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnext_s\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlegal_moves\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_buffer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     31\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mreward_clip\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     32\u001B[0m             \u001B[0mr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msign\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_11452\\1665206245.py\u001B[0m in \u001B[0;36mtrain_agent\u001B[1;34m(self, batch_size, num_games, reward_clip)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mtrain_agent\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m32\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_games\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreward_clip\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 30\u001B[1;33m         \u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnext_s\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlegal_moves\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_buffer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     31\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mreward_clip\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     32\u001B[0m             \u001B[0mr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msign\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_37_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_37_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_37_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_37_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_37_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_37_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_37_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_37_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.1\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py\u001B[0m in \u001B[0;36mstop\u001B[1;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[0m\n\u001B[0;32m    167\u001B[0m         \u001B[0mframe\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msuspend_jupyter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmain_debugger\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstep_cmd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    168\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 169\u001B[1;33m             \u001B[0mmain_debugger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    170\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    171\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.1\\plugins\\python\\helpers\\pydev\\pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1158\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1159\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1160\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1161\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1162\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.1\\plugins\\python\\helpers\\pydev\\pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1173\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1174\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1175\u001B[1;33m                 \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.01\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1176\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1177\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# This is where the magic happens.\n",
    "# This is where the other methods of the DeepQLearningAgent has to function as intended.\n",
    "# training loop\n",
    "\n",
    "# Initializing dict for logs\n",
    "model_logs = {'iteration': [], 'reward_mean': [],\n",
    "              'length_mean': [], 'games': [], 'loss': []}\n",
    "\n",
    "for index in tqdm(range(episodes)): # tqdm: related to loading bar, not of particular significance\n",
    "\n",
    "    # make small changes to the buffer and slowly train\n",
    "    # This should run as intended, no need to make changes here\n",
    "    _, _, _ = play_game2(env, agent, n_actions, epsilon=epsilon,\n",
    "                         n_games=n_games_training, record=True,\n",
    "                         sample_actions=sample_actions, reward_type=reward_type,\n",
    "                         frame_mode=True, total_frames=n_games_training,\n",
    "                         stateful=True)\n",
    "\n",
    "    # First encounter with train_agent\n",
    "    loss = agent.train_agent(batch_size=64,\n",
    "                             num_games=n_games_training, reward_clip=True)\n",
    "\n",
    "    if (index + 1) % log_frequency == 0:\n",
    "        agent.update_target_net()\n",
    "        agent.save_model(file_path='models/{:s}'.format(version), iteration=(index + 1))\n",
    "\n",
    "        # keep some epsilon alive for training\n",
    "        epsilon = max(epsilon * decay, epsilon_end)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
