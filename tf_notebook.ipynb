{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from utils import play_game2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense\n",
    "from tensorflow.keras import Model\n",
    "from agents.agent import Agent\n",
    "from agents.agent import mean_huber_loss\n",
    "from game_environment import SnakeNumpy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Parameters as from the config file of v17.1\n",
    "version = 'v17.1'\n",
    "board_size = 10\n",
    "n_actions = 4\n",
    "frames = 2\n",
    "supervised = False\n",
    "obstacles = False\n",
    "max_time_limit = 998\n",
    "buffer_size = 80000\n",
    "frame_mode = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class DeepQLearningAgent(Agent):\n",
    "    def __init__(self, board_size, frames, buffer_size, n_actions, version, use_target_net=True, gamma=0.99):\n",
    "        super().__init__(board_size, frames, buffer_size, gamma, n_actions, use_target_net, version)\n",
    "        self._model = self.model()\n",
    "        self._target_net = self._model\n",
    "        self.update_target_net()\n",
    "\n",
    "    # Manually implpementing the same model as in the v17.1 setup\n",
    "    def model(self):\n",
    "        input_board = Input((10, 10, 2,), name='input')\n",
    "        model = Sequential()\n",
    "        model.add(input_board)\n",
    "        model.add(Conv2D(filters=16, kernel_size=[3, 3], input_shape=(10, 10, 2),\n",
    "                         activation='relu', data_format='channels_last',\n",
    "                         padding='same', ))\n",
    "        model.add(Conv2D(filters=32, kernel_size=[3, 3], input_shape=(10, 10, 16),\n",
    "                         activation='relu', data_format='channels_last', ))\n",
    "        model.add(Conv2D(filters=64, kernel_size=[5, 5], input_shape=(8,8, 32),\n",
    "                         activation='relu', data_format='channels_last', ))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=64, activation='relu', name='action_prev_dense'))\n",
    "        model.add(Dense(units=4, activation='linear', name='action_values'))\n",
    "        model.compile(optimizer=RMSprop(0.0005), loss=mean_huber_loss)\n",
    "        return model\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self._target_net.set_weights(self._model.get_weights())\n",
    "\n",
    "    def train_agent(self, batch_size=32, num_games=1, reward_clip=False):\n",
    "        s, a, r, next_s, done, legal_moves = self._buffer.sample(batch_size)\n",
    "        if reward_clip:\n",
    "            r = np.sign(r)\n",
    "        # calculate the discounted reward, and then train accordingly\n",
    "        current_model = self._target_net if self._use_target_net else self._model\n",
    "        next_model_outputs = self._get_model_outputs(next_s, current_model)\n",
    "        # our estimate of expexted future discounted reward\n",
    "        discounted_reward = r + (self._gamma * np.max(\n",
    "            np.where(legal_moves == 1, next_model_outputs, -np.inf),\n",
    "            axis=1).reshape(-1, 1)) * (1 - done)\n",
    "        # create the target variable, only the column with action has different value\n",
    "        target = self._get_model_outputs(s)\n",
    "        # we bother only with the difference in reward estimate at the selected action\n",
    "        target = (1 - a) * target + a * discounted_reward\n",
    "        # fit\n",
    "        loss = self._model.train_on_batch(self._normalize_board(s), target)\n",
    "        # loss = round(loss, 5)\n",
    "        return loss\n",
    "\n",
    "    def _get_model_outputs(self, board, model=None):\n",
    "        # to correct dimensions and normalize\n",
    "        board = self._prepare_input(board)\n",
    "        # the default model to use\n",
    "        if model is None:\n",
    "            model = self._model\n",
    "        model_outputs = model.predict_on_batch(board)\n",
    "        return model_outputs\n",
    "\n",
    "    def _prepare_input(self, board):\n",
    "        if (board.ndim == 3):\n",
    "            board = board.reshape((1,) + self._input_shape)\n",
    "        board = self._normalize_board(board.copy())\n",
    "        return board.copy()\n",
    "\n",
    "    def _normalize_board(self, board):\n",
    "        return board.astype(np.float32) / 4.0\n",
    "\n",
    "    def save_model(self, file_path='', iteration=None):\n",
    "        if iteration is not None:\n",
    "            assert isinstance(iteration, int), \"iteration should be an integer\"\n",
    "        else:\n",
    "            iteration = 0\n",
    "        self._model.save_weights(\"{}/model_{:04d}.h5\".format(file_path, iteration))\n",
    "        if self._use_target_net:\n",
    "            self._target_net.save_weights(\"{}/model_{:04d}_target.h5\".format(file_path, iteration))\n",
    "\n",
    "    def move(self, board, legal_moves, value=None):\n",
    "        # use the agent model to make the predictions\n",
    "        model_outputs = self._get_model_outputs(board, self._model)\n",
    "        return np.argmax(np.where(legal_moves == 1, model_outputs, -np.inf), axis=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_27 (Conv2D)           (None, 10, 10, 16)        304       \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 8, 8, 32)          4640      \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 4, 4, 64)          51264     \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "action_prev_dense (Dense)    (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "action_values (Dense)        (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 122,068\n",
      "Trainable params: 122,068\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# This part is pretty much demystified\n",
    "# when DeepQLearningAgent is instanciated, it creates initial models based on the input of the given parameters including version for the model to use.\n",
    "agent = DeepQLearningAgent(\n",
    "    board_size=board_size, frames=frames, buffer_size=buffer_size, n_actions=n_actions,\n",
    "    version=version)\n",
    "agent.model().summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# some more funny parameters\n",
    "epsilon, epsilon_end = 1, 0.01\n",
    "reward_type = 'current'\n",
    "sample_actions = False\n",
    "n_games_training = 8 * 16\n",
    "games_eval = 8\n",
    "decay = 0.97\n",
    "episodes = 1 * (10 ** 3)\n",
    "log_frequency = 250"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing 32768 frames took 0.78s\n"
     ]
    }
   ],
   "source": [
    "# play some games to fill buffer\n",
    "games = 512\n",
    "env = SnakeNumpy(board_size=board_size, frames=frames,\n",
    "                 max_time_limit=max_time_limit, games=games,\n",
    "                 frame_mode=True, obstacles=obstacles, version=version)\n",
    "ct = time.time()\n",
    "_ = play_game2(env, agent, n_actions, n_games=games, record=True,\n",
    "               epsilon=epsilon, verbose=True, reset_seed=False,\n",
    "               frame_mode=True, total_frames=games * 64)\n",
    "print('Playing {:d} frames took {:.2f}s'.format(games * 64, time.time() - ct))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "env = SnakeNumpy(board_size=board_size, frames=frames,\n",
    "                 max_time_limit=max_time_limit, games=n_games_training,\n",
    "                 frame_mode=True, obstacles=obstacles, version=version)\n",
    "env2 = SnakeNumpy(board_size=board_size, frames=frames,\n",
    "                  max_time_limit=max_time_limit, games=games_eval,\n",
    "                  frame_mode=True, obstacles=obstacles, version=version)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 117/1000 [00:03<00:14, 61.99it/s]C:\\Users\\carlo\\miniconda3\\envs\\graded_2_pytorch\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "100%|██████████| 1000/1000 [00:18<00:00, 53.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "model_logs = {'iteration': [], 'reward_mean': [],\n",
    "              'length_mean': [], 'games': [], 'loss': []}\n",
    "\n",
    "for index in tqdm(range(episodes)):\n",
    "    # make small changes to the buffer and slowly train\n",
    "    _, _, _ = play_game2(env, agent, n_actions, epsilon=epsilon,\n",
    "                         n_games=n_games_training, record=True,\n",
    "                         sample_actions=sample_actions, reward_type=reward_type,\n",
    "                         frame_mode=True, total_frames=n_games_training,\n",
    "                         stateful=True)\n",
    "\n",
    "    loss = agent.train_agent(batch_size=64,\n",
    "                             num_games=n_games_training, reward_clip=True)\n",
    "\n",
    "    if (index + 1) % log_frequency == 0:\n",
    "        agent.update_target_net()\n",
    "        agent.save_model(file_path='models/{:s}'.format(version), iteration=(index + 1))\n",
    "\n",
    "        # keep some epsilon alive for training\n",
    "        epsilon = max(epsilon * decay, epsilon_end)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
